{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils.rl_glue import RLGlue\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gym\n",
    "import sys\n",
    "from Utils import envVisual\n",
    "from Agents import agent\n",
    "from Agents.QLearningAgent import QLearningAgent\n",
    "from Agents.ExpectedSarsaAgent import ExpectedSarsaAgent\n",
    "from Environments import gridworld_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_state_visits(state_visits, title):\n",
    "    grid_size = int(np.sqrt(len(state_visits)))\n",
    "    state_visits_grid = np.array(state_visits).reshape((grid_size, grid_size))\n",
    "\n",
    "    plt.imshow(state_visits_grid, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='State Visits')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = gridworld_env.Environment()\n",
    "base_env.env_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_visualizer = envVisual.EnvironmentVisualizer(base_env)\n",
    "env_visualizer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = base_env\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 25, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "agent = QLearningAgent()\n",
    "agent.agent_init(agent_info)\n",
    "\n",
    "for i_episode in range(1):\n",
    "    total_reward = 0.0\n",
    "    num_steps = 1\n",
    "    last_state = environment.env_start()\n",
    "    last_action = agent.agent_start(last_state)\n",
    "    observation = (last_state, last_action)\n",
    "    while True:\n",
    "        (reward, last_state, term) = environment.env_step(last_action)\n",
    "        total_reward += reward\n",
    "\n",
    "        env_visualizer.visualize()\n",
    "        \n",
    "        if term:\n",
    "            agent.agent_end(reward)\n",
    "            roat = (reward, last_state, None, term)\n",
    "            print('End game! Reward: ', total_reward)\n",
    "            break\n",
    "        else:\n",
    "            num_steps += 1\n",
    "            last_action = agent.agent_step(reward, last_state)\n",
    "            roat = (reward, last_state, last_action, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = base_env\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 25, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n",
    "agent = ExpectedSarsaAgent()\n",
    "agent.agent_init(agent_info)\n",
    "\n",
    "for i_episode in range(1):\n",
    "    total_reward = 0.0\n",
    "    num_steps = 1\n",
    "    last_state = environment.env_start()\n",
    "    last_action = agent.agent_start(last_state)\n",
    "    observation = (last_state, last_action)\n",
    "    while True:\n",
    "        (reward, last_state, term) = environment.env_step(last_action)\n",
    "        total_reward += reward\n",
    "\n",
    "        env_visualizer.visualize()\n",
    "        \n",
    "        if term:\n",
    "            agent.agent_end(reward)\n",
    "            roat = (reward, last_state, None, term)\n",
    "            print('End game! Reward: ', total_reward)\n",
    "            break\n",
    "        else:\n",
    "            num_steps += 1\n",
    "            last_action = agent.agent_step(reward, last_state)\n",
    "            roat = (reward, last_state, last_action, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"Q-learning\": QLearningAgent,\n",
    "    \"Expected Sarsa\": ExpectedSarsaAgent\n",
    "}\n",
    "\n",
    "env = gridworld_env.Environment\n",
    "all_reward_sums = {} # Contains sum of rewards during episode\n",
    "all_state_visits = {} # Contains state visit counts during the last 10 episodes\n",
    "agent_info = {\"num_actions\": 4, \"num_states\": 25, \"epsilon\": 0.1, \"step_size\": 0.5, \"discount\": 1.0}\n",
    "env_info = {}\n",
    "num_runs = 100 # The number of runs\n",
    "num_episodes = 500 # The number of episodes in each run\n",
    "\n",
    "for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "    all_reward_sums[algorithm] = []\n",
    "    all_state_visits[algorithm] = []\n",
    "    for run in tqdm(range(num_runs)):\n",
    "        agent_info[\"seed\"] = run\n",
    "        rl_glue = RLGlue(env, agents[algorithm])\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "        reward_sums = []\n",
    "        state_visits = np.zeros(25)\n",
    "#         last_episode_total_reward = 0\n",
    "        for episode in range(num_episodes):\n",
    "            if episode < num_episodes - 10:\n",
    "                # Runs an episode\n",
    "                rl_glue.rl_episode(0)\n",
    "            else:\n",
    "                # Runs an episode while keeping track of visited states\n",
    "                state, action = rl_glue.rl_start()\n",
    "                state_visits[state] += 1\n",
    "                is_terminal = False\n",
    "                while not is_terminal:\n",
    "                    reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                    state_visits[state] += 1\n",
    "\n",
    "            reward_sums.append(rl_glue.rl_return())\n",
    "#             last_episode_total_reward = rl_glue.rl_return()\n",
    "\n",
    "        all_reward_sums[algorithm].append(reward_sums)\n",
    "        all_state_visits[algorithm].append(state_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "    plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "plt.xlim(0,500)\n",
    "plt.ylim(-100,0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"Q-learning\", \"Expected Sarsa\"]\n",
    "num_algorithms = len(algorithms)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, algo in enumerate(algorithms, start=1):\n",
    "    plt.subplot(1, num_algorithms, i)\n",
    "    average_state_visits = np.array(all_state_visits[algo][-10:]).mean(axis=0)\n",
    "    title = f'State Visit Counts ({algo})'\n",
    "    visualize_state_visits(average_state_visits, title)  # Use average_state_visits here instead of all_state_visits_for_algorithm[-1]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
